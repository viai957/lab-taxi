{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\" Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - nA: number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "        \n",
    "    def epsilon_greedy(Q, state, nA, eps):\n",
    "        \"\"\"Selects epsilon-greedy action for supplied state\n",
    "        parameters :-\n",
    "        Q(dict): action-value function\n",
    "        state(int): current states\n",
    "        nA(int): number of actions in the environment\n",
    "        eps(float): epsilon\"\"\"\n",
    "        if random.random() > eps: #selects a greedy acion with epsilon\n",
    "            return np.argmax(Q[state])\n",
    "        else:\n",
    "            return random.choice(np.arange(nA))\n",
    "        \n",
    "    def updted_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None):\n",
    "        \"\"\"Returns updted Q-value for the most recent experience.\"\"\"\n",
    "        current = Q[state][action] # estimate in the Q-table (for current state, action pair)\n",
    "        # get value of state action pair at time step\n",
    "        Qsa_next = Q[next_state][next_action] if next_state is not None else 0\n",
    "        target = reward + (gamma * Qsa_next)\n",
    "        new_value = current + (alpha * (target - current))\n",
    "        return new_value\n",
    "        \n",
    "        \n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        - action: an integer, compatible with the task's action space\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.nA)\n",
    "\n",
    "    def step(self,num_episodes, state, action, reward, next_state, done, plot_every =100):\n",
    "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the previous state of the environment\n",
    "        - action: the agent's previous choice of action\n",
    "        - reward: last reward received\n",
    "        - next_state: the current state of the environment\n",
    "        - done: whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        Q = defaultdict(lambda: np.zeros(nA))\n",
    "        #monitor performence\n",
    "        temp_score = deque(maxlen=plot_every)\n",
    "        avg_scores = deque(maxlen=num_episodes)\n",
    "        for  i_episode in range(1, num_episodes+1):\n",
    "            # monitor progress\n",
    "            if  i_episode % 100 == 0:\n",
    "                print(\"/r Episode {}/{}\".format(i_episode, num_episodes),end=\"\")\n",
    "                score = 0\n",
    "                state = 0\n",
    "                eps = 1.0 / i_episode\n",
    "                action = epsilon_greedy(Q, state, nA, eps)\n",
    "                \n",
    "                while True:\n",
    "                    next_state, reward, done, info = action\n",
    "                    score += reward\n",
    "                    if not done:\n",
    "                        next_action = epsilon_greedy(Q, next_state, nA, eps)\n",
    "                        Q[state][action] = update_Q_sarsa(alpha, gamma, Q, \\\n",
    "                                                         state, action, reward, next_state, next_action)\n",
    "                        tmp_scores.append(score)\n",
    "                        break\n",
    "                    if (i_episode % plot_every == 0):\n",
    "                        avg_scores.append(np.mean(tmp_scores))\n",
    "        self.Q[state][action] += 1\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
